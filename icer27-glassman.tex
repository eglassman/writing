%\documentclass{sig-alt-release2}
%\documentclass{sig-alternate-2013}
\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[nodayofweek]{datetime}
\usepackage{url}
\usepackage{color}
\usepackage{balance}

%\pdfpagewidth=8.5in
%\pdfpageheight=11in
%
%\begin{document}
%\conferenceinfo{ICER'13,} {August 12--14, 2013, San Diego, California, USA.}
%\CopyrightYear{2013}
%\crdata{978-X-XXXX-XXXX-X/XX/XX}
%
%
%\clubpenalty=10000
%\widowpenalty = 10000

\newfont{\mycrnotice}{ptmr8t at 7pt}
\newfont{\myconfname}{ptmri8t at 7pt}
\let\crnotice\mycrnotice%
\let\confname\myconfname%

%\permission{Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Copyright is held by the author/owner(s).}
%\conferenceinfo{ICER'13,}{August 12--14, 2013, San Diego, California, USA.} \\
%\copyrightetc{ACM \the\acmcopyr\ ...\$15.00 \\ http://dx.doi.org/10.1145/2493394.2493421}
%\crdata{978-1-4503-2243-0/13/08}


%
%\clubpenalty=10000 
%\widowpenalty = 10000

\begin{document}

%\pagestyle{empty}
\title{Dealing with Multiple Implementations of Design Problems: \\Mining Student-Generated Alternatives}

%\numberofauthors{1}
\author{Elena Glassman \\ CSAIL HCI \\ ELG@MIT.edu
}

\maketitle

\begin{abstract}
In engineering design courses, many problems have a specification that the student's implementation must meet, but give the student a broad range of freedom for the internal design of that implementation. There may be several distinct, correct solutions, some of which may be unknown to the teaching staff or intelligent tutor designer. Visualization and classification of the multiple solutions that students generate in response to assigned engineering design problems will improve hints and answers to students' questions, whether they are provided by peers, staff, or automation. The approach will be to visualize hundreds or thousands of student implementations to discover alternatives, and then classify the implementations, and the paths that students take to each implementation, either by machine learning or human staff. The classifications will inform assistance given to students, in the form of staff-student discussions, peer-pairing, and/or automated help.% I log incremental snapshots of students' solutions as they progress toward correct and incorrect solutions. Initial investigations demonstrate that the choice of features to represent solutions is critical, and may be domain- or problem-dependent.
\end{abstract}

%\category{K.3.2}{Computers and Education} {Computers and Information Science
%  Education} [Computer Science Education]. See \cite{acm_classification:2013} for help using the ACM classification system.
%\terms {Measurement, Experimentation}
%
%\keywords{Choose your own specific keywords.}

%\category{K.3.2}{Computers and Education}{Computer and Information Science Education}[computer science education]
%
%\keywords{Problem Solving Process, Pattern Recognition}

%add categories about design-trade-offs, online learning, digital design
%\terms{Measurement, Performance, Human Factors} %Theory}
%\keywords{Pattern Recognition} %Problem Solving Process, Classification, Clustering, Intelligent Tutor, Student Progress Model, Similarity Metrics, Visual Data Mining, Programmer Event Data} %ACM proceedings, \LaTeX, text tagging}


\section{INTRODUCTION}

%I am a doctoral candidate in MIT's Electrical Engineering and Computer Science (EECS) Department. I have completed all of my qualifying exams, and am now in my fourth year. I discuss my initial thesis research investigations with several professors, including my advisor, Rob Miller. Together, they cover the fields of machine learning, human computer interfaces, and instructional design. I intend to formalize the relationship by the end of this summer, with the submission of my thesis proposal. I plan to analyze and present the data I collect from students over the next two years, in order to graduate in May 2015.

In engineering design courses, many problems have a specification that the student's implementation must meet, but give the student a broad range of freedom for the internal design of that implementation. There may be several distinct, correct solutions, some of which may be unknown to the teaching staff or intelligent tutor designer. This raises problems for helping students and giving feedback. In face-to-face situations, if a teaching assistant doesn't recognize the student's implementation path, then they may redirect the student completely, costing them work and possibly derailing a novel, valid solution. In an intelligent tutor or massively open online course (MOOC), the automated hint generators may not recognize the unexpected solution paths, and will generate unhelpful hints.

In order to reduce confusion, I must lay out a few definitions. In this proposal, I define an implementation to be piece of code that a particular person wrote. I define a solution to be more abstract than an implementation: while there may be an implementation submitted by each student in the class, there may be only two distinct solutions, representing two different approaches or strategies to achieving the same input-output behavior. I define an implementation path to be a series of code snapshots generated by a person working toward meeting a particular input-output behavior specification. Likewise, I define a solution path to be more abstract, referring to a general path that one or several implementation paths may trace out on the way to a working implementation. The ``space of student solutions'' refers to the aggregation of student-generated implementations and the variety of solutions they represent.

There is a growing set of recent papers documenting methods for discovering the space of student-generated solutions. A common goal of this existing research is to help teachers monitor the state of their class, or enable them to give solution-specific feedback to more students. The proposed work will contribute to that literature, as well as explore how it can be integrated with other systems, such as peer-assistance platforms and automated help.


 %In the Matlab challenge, visualizing code parse tree size provides insight into common strategies as well as successful and unsuccessful outliers. 
%

%In CompArch, this led to better education of the teaching staff. As a result, I now ask a simple question to identify the student's approach before trying to help them.

%\section{MOTIVATING EXAMPLE}
%I am currently an instructor for an undergraduate introductory course on computer architecture (CompArch). Enrollment has swung between roughly two hundred and over three hundred students per semester. One mid-semester lab assignment in this course requires that students create state-transition rules for a Turing machine. %Each line in the behavioral specification boils down to the following: if you're in state \textbf{X} and you read symbol \textbf{Y} on the tape, overwrite symbol \textbf{Y} with symbol \textbf{Z}, move the tape-reader in direction \textbf{W}, and transition to state \textbf{Q}.
%Many distinct sets of state-transition rules behave identically, given the same input tape. %Even after looking at hundreds of Turing machines, which all give the same correct final answers, there is very little a human can discern simply by looking at the students' code. The same is true even after translating these textual statements into a diagram of state transitions.
%
%The dynamic behavior of students' Turing machines has recurring patterns. I visualized this dynamic behavior for 148 students' two-state Turing machines, and by visual inspection of the Turing machine's movement across a common input tape, identified strategies \cite{ICERGlassman}. The majority (88\%) of the solutions employed one of two mutually exclusive strategies: (1) matching the innermost open parenthesis with the innermost closed parenthesis and (2) matching the $n^{th}$ open parenthesis with the $n^{th}$ closed parenthesis.
%
%The fact that there were {\em two} correct solutions to the Turing machine assignment came as a surprise. I now ask struggling students a question first, to determine the solution they are aiming for. I can then suggest edits that preserve the chosen solution, if I recognize it as correct. Common solution identification also allows me to recognize novel alternative solutions.
%
%%Each staff member for this course is encouraged to complete the lab on their own before counseling students. Staff members were aware of the solution they each found, and yet were not aware that there were two mutually exclusive common solutions. 
%
%This is one example of an engineering design problem where a small number of common, distinct, correct solutions are distinguishable once an appropriate representation of the data is found. My thesis research explores the generality of this phenomenon, with a focus on scaling up to environments with hundreds or thousands of students.


\section{BACKGROUND \& RELATED WORK}

The related work has been partitioned based on method of discovering the space of solutions, existing structures within learning environments that this knowledge could enhance, and the specific domain to which these methods and interventions are applied. The work highlighted in the first section below addresses the feasibility and current state-of-the-art in classifying student implementations and implementation paths, using machine learning algorithms and visualization methods. The second section highlights work from the Computer Science Education community on the relevance of solution space knowledge to various methods used within learning environments. The final subsection surveys the existing domains in which these methods have been applied, specifically concerning the type and scale of programming challenges.

\subsection{Classification of Student Implementations}

The most recent relevant work on finding clusters in implementations comes from Stanford. In \cite{MOOCshop}, Huang et. al. consider tens of thousands of implementations submitted to Stanfordâ€™s Fall 2011 Machine Learning MOOC, and identify clusters of implementations based on measures of syntactic and functional similarity. 

Huang et. al. \cite{MOOCshop} make a case for mapping out the solution space using analysis beyond just input-output behavior. They claim that output-based feedback alone is insufficient, since the relationship between input-output pairs and bugs, both mental and programmatic, is not a one-to-one mapping.  They observed that for students' approaches to implementing regularized logistic regression, {\em similarly behaving programs were implemented in significantly different ways}. While Huang et. al. focused on how to get appropriate teacher feedback to each student, they conclude that knowing the space of solutions, and how current students are distributed in that space, is valuable to the teaching staff, in both residential and online courses. 

The results presented in Huang et. al. corroborate earlier results on the feasibility of automated classification of student code submissions. For example, Taherkhani et al. \cite{taherkhani12} demonstrated the practicality of identifying which sorting algorithm a student implemented, using supervised machine learning methods. 
 
Luxton-Reilly et al. \cite{Luxton13} takes a very different approach to discovering the kinds and degree of variation between student generated implementations. By thematic analysis \cite{thematic06} of student submissions, the authors generated a taxonomy that captures the variation between correct implementations in their dataset. They then created an Eclipse plug-in for classifying new code examples based on their taxonomy. 

%
%
%An active/interactive machine learning technique, which take advantage of human experts in the loop to resolve uncertainties, have been deployed for de-duplication in Stonebraker et al.'s Data Tamer \cite{DataTamer}.

\subsubsection{Classifying Paths to Implementations}

Visualization and automatic recognition of the multiple ways by which a student can approach a solution is an area of active research. Using automated classification methods, Piech et. al. \cite{Piech} found distinct development paths to working implementations by putting students' incremental attempts into a pipeline involving classification, milestone discovery, Hidden Markov Modeling of the students' process, and clustering of implementation paths. These identified paths are visualized as finite state machine transition diagrams. Evaluation focused on predicting midterm exam grades and detecting milestone difficulty.

The following examples highlight research that is further from relevance to this thesis because the paths to a working implementation are classified by behavior rather than the type of final solution found. They are included for context. Kiesmueller et al. \cite{Kiesmueller} attempted to recognize strategies at a very high level, which are not specific to the challenge at hand. Example high-level problem-independent strategies were a top-down or bottom-up programming style. Helminen et al. \cite{ICERHelminen} introduced novel interactive graphs for examining the problem solving process of students working on small programming-like problems. However, problems with multiple solutions were outside the scope of their investigation.

%
%\subsubsection{Active Learning of Solution Clusters}
%
%CITE DATA TAMER APPLICABILITY FOR ITS HUMAN ACTIVE LEARNING COMPONENT to partial solutions
%Community source the distance metrics/cluster boundaries: Ask Student: "Is this what you did?" Ask teacher: "Are these the same?"

\subsection{Relevant Learning Environment Methods}

\subsubsection{Comparing and Contrasting Examples}

%Discussions/compare and contrast: Patitsas et al. \cite{PatitsasICER13} has... (and look at her cited works, include them too!) 

%Since the Computer Science Education community sees great learning value in examples,  

%In order to communicate programming concepts, it is common practice for teachers to present example programs. Furthermore, it is common to present several different examples, to illustrate various approaches and styles \cite{Luxton13}. 

%Current educational reforms in mathematics advocate that the
%teacher act more as a facilitator, encouraging students to share and
%compare their own thinking and problem-solving methods with
%other students



Marton et al.'s variation theory \cite{Marton03} holds that in order to learn something, one must see examples that vary along particular dimensions: ``contrast,'' as in pairing it with something it is not; ``generalization,'' as in presenting multiple instances of the object or concept to be learned, varying only that which is irrelevant; ``separation,'' as in presenting multiple instances of the object or concept, varying only that which can vary internally without changing the object or concept into something else; and ``fusion,'' as in seeing multiple examples in which previously analytically separated aspects must be processed together to recognize the object or concept. The aspects which are related to these dimensions of variation and therefore define the object or concept are called ``critical features.''

Peer reviews and assessments, surveyed in \cite{peerReview98}, are one of the existing pedagogies in which teachers ask students to compare and constrast examples. The pedagogical method of comparing and contrasting ways of approaching a solution has now been validated in the literature of mathematics education research \cite{Star07}, cognitive science \cite{3,12,and13inPatitsas}, and computing education research \cite{Suhonen08, PatitsasICER13}.

Given Marton et al.'s rubric for effective patterns of variation, and the identification of ``critical features,'' one can discern between more or less theoretically effective examples of the object or concept given to a student to learn. On this basis, Luxton-Reilly et al. \cite{Luxton13} suggest that identifying distinct clusters of implementations can help instructors select appropriate examples of code for teaching purposes.



%More concretely, comparing and contrasting solution approaches Patitsas et al. \cite{PatitsasICER13} has 
%
% These methods have their theoretical basis in variation theory \cite{} and social cognitive theory \cite{}. 
%
%Comparing and contrasting dierent solution approaches is
%known in math education and cognitive science to increase
%student learning { what about CS? In this experiment, we
%replicated work from Rittle-Johnson and Star, using a pretest{
%intervention{posttest{follow-up design (n=241). Our intervention was an in-class workbook in CS2. A randomized half
%of students received questions in a compare-and-contrast
%style, seeing dierent code for dierent algorithms in parallel. The other half saw the same code questions sequentially,
%and evaluated them one at a time. Students in the former
%group performed better with regard to procedural knowledge (code reading & writing), and 
%exibility (generating,
%recognizing & evaluating multiple ways to solve a problem).
%The two groups performed equally on conceptual knowledge.
%Our results agree with those of Rittle-Johnson and Star, indicating that the existing work in this area generalizes to CS
%education.    
%
%In light of these pedagogical frameworks, Luxton-Reilly et al. \cite{Luxton13} suggest that identifying distinct clusters of solutions can help instructors select appropriate examples of code for teaching purposes.

\subsubsection{Feedback to Students}

Peer-pairing can stand in place of staff assistance, to both reduce the load on teaching staff and give students a chance to gain ownership of material through teaching it to someone else. Weld et al. speculate about peer-pairing in MOOCs based on student competency measures \cite{WeldHcomp12}, and Klemmer et al. demonstrates peer assessments' scalability to large online classes \cite{Klemmer}.

Generating tailored feedback to students in large classes tackling problems even as short as introductory programming assignments requires many man-hours of repetitive work. Singh et al. \cite{rishabh} are pushing the state of the art of automated feedback for short introductory programming assignments. However, their software is currently only differentiating between implementations based on their input-output characteristics. For example, this system cannot currently differentiate between two different sorting algorithms. If there are common dead-ends that have been identified by looking at incorrect student implementations to a particular problem, by hand, this system can identify that a student is very close to a known dead-end approach, but it cannot identify {\em which} functionally equivalent variant of a correct implementation a student is approaching. 

If Singh et al.'s automated feedback represents the one extreme end of the spectrum for providing tailored feedback to students, Huang et. al. \cite{MOOCshop} represents the other end. By clustering syntactically similar solutions which fail on the same input-output tests, Huang et al. hope to enable the ``force multiplication'' of teacher feedback, where a hand-written critique can be sent to each entire cluster.

%\subsection{Domain of Application}
%
%LOOK AT COMPARCH COMMUNITY, describe scale of Singh solution, MOOCshop solution, types of programming (languages)

\section{STATEMENT OF THESIS/PROBLEM}

My thesis statement is: Visualization and classification of the multiple solutions that students generate in response to assigned engineering design problems will improve hints and answers to students' questions, whether they are provided by peers, staff, or automation. 

I plan to explore the following aspects of this claim:
\begin{itemize}

\item What features are useful for visualizing or automatically classifying alternative implementations?

\item How do we get students to think about alternatives not taken?

\item How can peers help each other when there are multiple good implementations?

\item How can we provide automated help based on archived implementations?

%\item What features are useful for visualizing or automatically clustering engineering design solution paths? For programming domains, for example, features could include measures of program complexity, stack depth, and runtime characteristics. For digital logic and analog circuit domains, features could include graph metrics and voltage traces on intermediate nodes.
%\item How can teaching staff be trained to quickly recognize the solution path of a given student, in order to give tailored feedback?
%\item If teaching staff are in short supply (as in a MOOC), how can peers help each other in a space where there are multiple good solution paths?
%\item If peer help is not feasible, then how can we provide automated help based on solution path recognition in a design space where multiple correct paths are possible?
\end{itemize}


\section{RESEARCH GOALS \& METHODS}

The approach will be to visualize hundreds or thousands of student implementations to discover alternatives, and then classify the implementations, and the paths that students take to each implementation, either by machine learning or human staff. The classifications will inform assistance given to students, in the form of staff-student discussions, peer-pairing, and/or automated help.

\subsection{Enabling Exploratory Data Analysis and Clustering}

As a researcher, my first step toward discovering the space of correct solutions for a given assignment will be to look at the raw data. I have access to anonymized code submissions from the Spring 2013 semester of both a virtual hardware (6.004) and software (6.005) design class. By poring over many examples of students' code, I can get a sense for what design decisions partition the space of implemented solutions into useful clusters. Useful, in this case, refers to clusters which help me conceptualize the space of solutions and identify where new students fall into that space. The intuition gained through this first exploratory step can guide the steps that follow.

A next step toward discovering the space of correct solutions is visualizing many student solutions together. The final choice of features and graphical representation is the result of exploratory data analysis. I have already taken this approach to multiple labs in 6.004, an undergraduate computer architecture course in which virtual hardware is designed at the MOSFET and CMOS gate level. I have also observed this approach in a collaborator's work on an online Matlab programming challenge. Plotting dynamic behavior or static features such as parse tree size is enough, in these initial examples, to separate students' solutions into clear clusters representing different solutions.

By repeating this process on several domains, specifically on 6.004's design assignments written in a hardware description language and on 6.005's software designs written in Java, I propose to generalize the experience and build a tool for teachers which supports this kind of exploratory analysis of student solutions regardless of the teachers' specific domain and assignment. The eligible space of domains this could potentially generalize to are ones in which implementations are submitted as machine-readable code and where assignments have a specification that the student's implementation must meet, but give the student a broad range of freedom for the internal design of that implementation.

The exploratory data analysis and feature selection will power machine learning tools, such as clustering algorithms, potentially including active/interactive machine learning modules for teachers to correct the systems' classification of new implementations. 

In order to evaluate the contribution of this interactive data visualization environment for teaching staff, I will run a user study on staff members of courses, at MIT and possibly other institutions, in which the student submissions are in machine-readable code and there is non-trivial latitude given to students for the internal implementation design. By interacting with representations of many students solutions' at once, users will attempt to identify clusters. If staff independently find similarly meaningful, persistent patterns in their students' solutions using the tool, I will consider it a success.

\subsection{Solution Path Recognition}

%My approach will be to visualize hundreds or thousands of student implementations to discover alternatives, and then classify the path that students take to each implementation, either by machine learning or human staff. The classifications will inform assistance given to students, in the form of staff-student discussions, peer-pairing, and automated help. 

%There are three main variables within this thesis. The first is domain, as in whether the data is generated by a hardward or software course, and the scale of student implemementations. The second variable is the method of classification: are humans interpreting visualizations or are machine learning algorithms doing the bulk of the classification. Finally, there is the application of this generated knowledge. It can be used to facilitate design review discussions between staff and students, pair peers, or improve automated feedback.

%I will run a user study to access whether the interactive data visualization environment I design helps teaching staff find meaningful strategy distinctions. My subjects will be teaching staff of engineering courses. If subjects use the tool and find similar demarcations between strategies, and possibly similar sets of features to describe those demarcations, it may be possible to conclude the tool itself is enabling the staff to identify meaningful, persistent patterns in their students' solutions. %In a pre- and post-interview, we can analyze how the tool affects the staff's understanding of the ways in which a student can solve a particular assigned engineering problem.

For peer-pairing, there are a variety of solution path-dependent pairing strategies. One could pair students on the same solution path: students who have implemented a recognized solution would be paired with another student who is struggling to implement the same solution. By comparing metrics, e.g., grades, of students paired based on solution paths, relative to random pairing and no pairing, I can measure the interventions' educational value.

% The learning, satisfaction, and perception of self-efficacy gains associated with these pairing strategies can be tested in a study deployed within a residential or online engineering class, after integrating the existing question-and-answer/discussion forum with the learning management system that keeps track of each student's solution paths.

%For example, one could pair two students whose solution paths are heading toward the same solution among the multiple possibilities for a particular engineering design problem, but one has a bug that the other has already fixed in their own code. Alternatively, one could pair two students, one who has already successfully employed a particular strategy with one who is struggling to implement it. The learning, satisfaction, and perception of self-efficacy gains associated with these pairing strategies can be tested in a study deployed within a residential or online engineering class, after integrating the existing question-and-answer/discussion forum with the learning management system that keeps track of each student's history of solutions. By incentivising students to assist other students, and then pairing them randomly or according to solution path-dependent strategies, I can approximately measure the educational value of the intervention.
%
%In CompArch, we capture snapshots of students' intermediate code as they work. These snapshot sequences are solution paths ready for analysis. One semester's data has been collected already. I hope to demonstrate generality on similar datasets from 6.005, a software engineering course which also has collected snapshot sequences of students' code.

\section{EXPECTED CONTRIBUTIONS}

This research may be particularly helpful to staff who focus on helping each student reach their own envisioned solution. The algorithms, tools, visualizations, and resulting insights from this work are intended to support this constructivist approach to teaching, facilitate students helping each other, and inform automated feedback.

I expect to build software that help students reach their envisioned (valid) implementations, and also understand alternative design choices and their tradeoffs. At the same time, I expect to create a software which helps teachers across multiple domains discover the full space of alternatives and recognize novel or better implementations in their students' implementations.

%Since my thesis research is relevant to engineering design tasks with multiple correct solutions, it may be of particular interest to teaching staff with a constructivist perspective. Such staff members may be attempting to elicit the student's envisioned solution strategy, and then help them toward a working solution that employs that strategy. The algorithms, tools, visualizations, and resulting insights from this and future work are intended to augment this constructivist approach to teaching, facilitate students helping each other, and inform automated feedback.

\section{TIMELINE}

\begin{itemize}
\item {\bf Fall '13} Deploy server-based D3 visualizations of working student submissions in the context of last years' working submissions during check-offs with me in 6.004. Log interactions with the visualization and record conversations with students while they compare their design to other options. Explore the generalization of the visualizations to representing Java exercises completed in 6.005 during past semesters. Explore use of Rishabh Singh's autograder for JSIM, and note what gaps in feedback arise from its current sole consideration on input-output behavior.
\item {\bf Spring '14} Use Bose Fellowship to improve and deploy visualization web applications for all interested LAs and TAs, interviewing them on their knowledge of the solution space pre- and post-interaction with the visualization. Add machine learning components as necessary, for helping humans identify patterns. Explore use for partial as well as completed labs. Iterate with 6.005 staff on visualizations of Java exercises that suit their course and classroom needs. Work with Singh to augment his autograder so that it gives feedback informed by which of several implementations a student is "closest" to.
\item {\bf Summer '14} Do an internship outside Boston, probably focusing on the visual presentation of information.
\item {\bf Fall '15} Use Bose Fellowship to improve and deploy visualization web applications for all interested LAs and TAs, and auto-suggest peer-pairings for students based on partial implementations. Compare solution paths for students experiencing auto-suggested peer assistance versus staff assistance. Continue iterating with 6.005 staff on visualizations of Java exercises that suit their course and classroom needs. Deploy Singh's modified autograder in 6.004.
\item {\bf Spring '15} Write thesis. Tie up loose ends. Defend thesis. Celebrate. Graduate!
\end{itemize}

\section{Acknowledgments}
This work is supported by the National Science Foundation Graduate Research Fellowship under Grant No. 1122374 and the MIT Amar Bose Teaching Fellowship. It has also been made possible by countless discussions and pair-research sessions with fellow members of CSAIL HCI, Prof. Leslie Kaelbling, Prof. Chris Terman, and Marty Glassman.

\bibliographystyle{abbrv}
\bibliography{icerbib}

%\bibliographystyle{abbrvurl}
%%\bibliography{biblio} Edit the name of your bib file here and delete the refs below.
%\begin{thebibliography}{1}
%\bibitem{acrobat_reader:2013}
%Adobe {A}crobat {R}eader 7.
%\newblock URL: \url{http://www.adobe.com/products/acrobat/}.
%\bibitem{acm_classification:2013}
%How to {C}lassify {W}orks {U}sing {ACM}'s {C}omputing {C}lassification
%{S}ystem.
%\newblock URL: \url{http://www.acm.org/class/how_to_use.html}.
%\bibitem{anderson:1992}
%R.~Anderson.
%\newblock Social impacts of computing: Codes of professional ethics.
%\newblock {\em Social Science Computing Review}, 10(2):453--469, 1992.
%\end{thebibliography}
\end{document}

